[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/non-linear-dynamics-trajectories-with-python/index.html",
    "href": "posts/non-linear-dynamics-trajectories-with-python/index.html",
    "title": "Non-Linear Dynamics Trajectories with Python",
    "section": "",
    "text": "Non-Linear Dynamics Trajectories with Python\nby Ritobrata Ghosh\n\nIntroduction\nIn this Notebook, trajectories of the Logistic Map [1] is plotted with Python.\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef logistic_function(parameter: float, initial_value: float) -&gt; float:\n    return parameter * initial_value * (1 - initial_value)\ndef logistic_function_trajectory(parameter: float,\n                                 initial_value: float,\n                                 num_iter: int) -&gt; np.array:\n    trajectory = np.zeros(num_iter)\n    trajectory[0] = initial_value\n    for i in range(num_iter-1):\n        trajectory[i+1] = logistic_function(parameter, initial_value)\n        initial_value = trajectory[i+1]\n    return np.array(trajectory)\nlogistic_function_trajectory(parameter=2.2, initial_value=0.8, num_iter=10)\narray([0.8       , 0.352     , 0.5018112 , 0.54999278, 0.54450159,\n       0.54564314, 0.54541675, 0.5454621 , 0.54545303, 0.54545485])\nxs = np.linspace(1, 200, 200)\nys = logistic_function_trajectory(parameter=2.2, initial_value=0.8, num_iter=200)\ndef plot_trajectory(xs, ys, x_label, y_label, title):\n    plt.figure(figsize=(20,10))\n    plt.scatter(xs, ys, marker='o', edgecolor='black', c=point_colors * 50);\n    plt.xlabel(x_label);\n    plt.ylabel(y_label);\n    plt.title(title);\n    plt.grid();\npoint_colors = ['cyan', 'magenta', 'yellow', 'black']\nplot_trajectory(xs=xs, ys=ys, x_label='$n$', y_label='$x_n$', title='Trajectory')\n\n\n\n\\[r = 2\\]\nx_n = logistic_function_trajectory(parameter=2.0, initial_value=0.2, num_iter=200)\nx_n_hat = logistic_function_trajectory(parameter=2.0, initial_value=0.200001, num_iter=200)\nys = abs(np.subtract(x_n, x_n_hat))\nxs = np.linspace(1, 200, 200)\nplot_trajectory(xs=xs, ys=ys, x_label='$n$', y_label='$ x_n - \\hat{x_n} $', title='Trajectory for $r = 2$')\n\n\n\nPlotting \\(x_n - \\hat{x_n}\\) vs. \\(n\\) for \\(r=3.4\\)\nx_n = logistic_function_trajectory(parameter=3.4, initial_value=0.2, num_iter=200)\nx_n_hat = logistic_function_trajectory(parameter=3.4, initial_value=0.200001, num_iter=200)\n\nys = abs(np.subtract(x_n, x_n_hat))\nxs = np.linspace(1, 200, 200)\n\nplot_trajectory(xs=xs, ys=ys, x_label='$n$', y_label='$ x_n - \\hat{x_n} $', title='Trajectory for $r = 3.4$')\n\nWe can thus plot the trajectories of the Logistic Map with Python and relevant libraries.\n\n\n\nReferences\n\nhttps://en.wikipedia.org/wiki/Logistic_map\nBradley, Liz: Nonlinear Dynamics: Mathematical and Computational Approaches via Complexity Explorer, Santa Fe Institute\n\nCite this Notebook:\n@ONLINE {,\n    author = \"Ritobrata Ghosh\",\n    title  = \"Non Linear Dynamics Trajectories with Python\",\n    month  = \"apr\",\n    year   = \"2023\",\n    url    = \"https://www.kaggle.com/truthr/non-linear-dynamics-trajectories-with-python\"\n}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nNon-Linear Dynamics Trajectories with Python\n\n\n\n\n\n\n\ndeep-learning\n\n\nphysics\n\n\nchaos-theory\n\n\nai\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2023\n\n\nRitobrata Ghosh\n\n\n\n\n\n\n  \n\n\n\n\nA Gentle Introduction to the Hugging Face API\n\n\nA Hands-On Tutorial\n\n\n\n\ndeep-learning\n\n\nmachine-learning\n\n\ndata-science\n\n\nai\n\n\ntutorial\n\n\nnlp\n\n\nkaggle\n\n\n\n\n\n\n\n\n\n\n\nJun 20, 2021\n\n\nRitobrata Ghosh\n\n\n\n\n\n\n  \n\n\n\n\nCreating Any Boolean Function\n\n\nHow to Represent Any Boolean Function with Elementary Logic Gates\n\n\n\n\nnand2tetris\n\n\ncomputer-science\n\n\nelectronics\n\n\ncomputation\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2021\n\n\nRitobrata Ghosh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/intro-to-huggingface-api/2021-06-20-intro-huggingface-api.html",
    "href": "posts/intro-to-huggingface-api/2021-06-20-intro-huggingface-api.html",
    "title": "A Gentle Introduction to the Hugging Face API",
    "section": "",
    "text": "Introduction\nNatural Language Processing is a fast-advancing field. And it is also one of the fields that require a huge amount of computational resources to make important progress. And although breakthroughs are openly announced, and papers are released in free-to-access repositories such as arXiv, Open Review, Papers with Code, etc., and despite (sometimes) having the code freely available on GitHub, using those language models is not something widely accessible and easy.\nLet me provide more context. BERT is a state-of-the-art encoder language model. It takes days to train the model from the ground up even when using very powerful GPUs that only a few entities have access to. In 2019, NVIDIA used 1472 NVIDIA V100 GPUs to train BERT from scratch in 53 minutes. Yes, 1,472!\nOne estimate puts the cost of training GPT-3, a 175 billion parameter model, for a single training run at $12 Million USD.\nAnd such language models are released every now and then. How do you use these powerful language models for your task?\nHere Hugging Face comes to the scene. They aim to solve this problem by providing pre-trained models, and simple API so that you can use them, fine-tune them, and use the API in your applications.\nIn this article, my goal is to introduce the Hugging Face pipeline API to accomplish very interesting tasks by utilizing powerful pre-trained models present in the models hub of Hugging Face.\nTo follow through this article, you need not have any prior knowledge of Natural Language Processing. I, however, assume minor prior experience in writing Python code.\n\nIn this article, I will go over, describe, and provide examples for the following tasks using Hugging Face pipeline-\n\nSentiment Analysis\nZero-Shot Classification\nText Generation\nMask-Filling\nNamed Entity Recognition\nQuestion Answering\nSummarization\nTranslation\n\n\n\n\nInstalling and Importing\n! pip install transformers[sentencepiece] &gt; /dev/null\nIf you have not seen a pip install with a square bracket in it before, don’t worry. It means that you are installing the transformers package with extra support added for the sentencepiece package. You can find out more about square parentheses installs in pip here.\nfrom transformers import pipeline\nWe will use the utilities available in the pipeline namespace in the transformers library.\n\n\n\nSentiment Analysis\nBefore I begin going through the specific pipelines, let me tell you something beforehand that you will find yourself. Hugging Face API is very intuitive. When you want to use a pipeline, you have to instantiate an object, then you pass data to that object to get result. Very simple! You are soon to see what I mean.\nclassifier_sentiment = pipeline(\"sentiment-analysis\")\nThat’s it. You call the pipeline() method with the task you want to accomplish as an argument. And you assign a name to it. You are done now. You can now begin to use the object as a function to achieve what you want. Let’s see an example-\n\n\nSee, it returns a dictionary contained in a list that has two items, label and score. The label part tells us its prediction, and the score tells us its confidence score.\n\nAs an aside, I think they are structured this way because this structure is easily compatible with .json and similar filetypes which are very common in APIs.\n\nLet’s see another example.\n\n\nSee, how the classifier is aware of my emotion about pineapple pizzas? 🙃\n\n\n\nZero-Shot Classification\nWhen you want to classify something using Deep Learning, in many cases you need to train it with labeled examples. This approach is known as Supervised Learning. Even when leveraging transfer learning, you need to train your model with quite a few labeled examples in the domain of your choice. Zero-shot classification is different than that. In this, you use a pre-trained model to classify a given string and some labels of your choice. The model returns you the confidence score for each model.\nLet me show you how this works. First, you instantiate an object and assign it a name just like you did in the case of the Sentiment Analysis pipeline.\nclassifier_zero_shot = pipeline(\"zero-shot-classification\")\nThen you pass a string, along with the labels of your choice to test how well they correspond to your sentence.\n\n\nSee, the given sentence is about a profession, and the model tells you that. And it knows that it is much more related to education than politics.\nEssentially, when you are doing Zero-Shot Classification, you are supplying a string to the pipeline, and also labels. The pipeline returns how accurate those labels are.\n\n\n\nText Generation\nBefore I heard about Deep Learning, I heard about Natural Language Processing, because I heard that you can generate text with it! This is the most exciting part of NLP to me, personally.\nAs the name suggests, this pipeline lets you generate text. It just needs you to supply a prompt, a text that will determine what the generated text will be. Let me show you how.\ngenerator = pipeline(\"text-generation\")\n\n\nSee what I mean! Even with zero training, it can create logically coherent text that is even interesting to read. Robots will take over writers soon 🤖!\nYou can control the maximum length of the output of the pipeline, and even ask for multiple possible outputs, like so-\n\n\n\nWhoa, it’s poetic!\n\nWe can accomplish more interesting things with transfer learning. But that is a story for another day.\n\n\n\nMask Filling\nIn this pipeline, if there is a word that you hide and supply the string with the hidden word, the pipeline will predict that word. This is like magic!\nunmasker = pipeline(\"fill-mask\")\nYou pass the sentence with the masked word, and it will predict it. You can choose the number of predictions you want to see. You just have to pass a value to the top_k parameter.\n\n\n\n\nIt can predict the words quite well. Just remember to mask the word using &lt;mask&gt;.\n\n\n\nNamed Entity Recognition\nThis pipeline recognizes the proper nouns in your sentence. And it also classifies them. Let’s see an example.\nner = pipeline(\"ner\", grouped_entities=True)\nThe grouped_entities parameter just ensures that the pipeline can recognize the names that have more than one word, like, you know, Chocolate Factory.\n\n\nSee how it recognizes a person, his profession, and location on its own, and also labels them as “PER”, “ORG”, and “LOC” (meaning location), respectively.\n\n\n\nQuestion Answering\nThis pipeline provides you with an answer to a question you ask from the context that you provide.\nquestion_answerer = pipeline(\"question-answering\")\n\n\nIt can answer multiple questions from the same prompt.\n\n\n\n\n\nSummarization\nThis is one of the most practical tasks that has existed for a long time. Deep Learning really changes the landscape of the task.\nLet’s see it in action.\nsummarizer = pipeline(\"summarization\")\nLet’s summarize a piece of text from a blog post I recently read.\n\n\nNow, that’s something! The efficacy of the summarizer really shocked me. It does this task so well.\n\n\n\nTranslation\nThe translation pipeline translates a piece of text from one language to another. It also lets you choose the language model of your choice.\nFirst, let’s translate a text from English to German. I will use the Helsinki-NLP/opus-mt-en-de model for this task.\ntranslator_de = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-de\")\n\n\nNote that you are not explicitly telling the model about the language of the input language or the output language. A language model is only trained for translating from one language to another. It cannot translate other languages. If you want to translate from German to English, you’d have to use another model from the model hub. You can find all the translation models here.\nThe cool thing is that Hugging Face is not limited to Romance languages or European languages in general. Let me show you by translating a piece of text from Bengali.\ntranslator_bn = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-bn-en\")\n\n\n\n\n\n\n\n\nConclusion\nI have shown you several pipeline APIs and what they do and how they work.\nHere are the suggested next steps-\n\nExperiment. Play with the pipelines by yourself, try different things with different parameters, run inference on examples of your own, test edge cases, run wild with it. I believe that is the best way to learn.\nDo Projects. Use the knowledge you gained here in simple projects. We only learn by doing.\nHugging Face Course. Hugging Face has recently released a free course where they teach you about Transformers and their library ecosystem. It is a good next step.\nExplore. Explore the Hugging Face website, learn more about their APIs, Services, pre-trained models, and many more things it has to offer.\n\n\nIf you notice any error, mistake, or typo, please feel free to let me know. If something is not clear, please let me know that as well. I will do my best to correct it and will credit you here.\nLet me know if you have any questions at all.\n\n(Originally published as a Notebook on Kaggle.)"
  },
  {
    "objectID": "posts/nand2tetris_1/2021-06-13-chips-logic-gates-nand2tetris-1.html",
    "href": "posts/nand2tetris_1/2021-06-13-chips-logic-gates-nand2tetris-1.html",
    "title": "Creating Any Boolean Function",
    "section": "",
    "text": "by Ritobrata Ghosh\n\nIntroduction\nDigital electronics is probably the most impactful and widespread technology now. It forms the basis of all of the chips- from ARM microcontrollers to processors in cellphones, all the way to uberpowerful AMD Threadrippers. In all these cases, digital electronics reign supreme.\nIt is very useful to learn not only about basic principles but also how these principles and the components of Digital Electronics exactly form the basis of the modern world, i.e., how do we go from basic logic gates to fully functional computers.\n\n\nElements of Computing Systems\nDigital Electronics is not only important because it is the base of the modern technology that literally governs our lives, but it makes us better as programmers, and computer scientists.\nWhether we are self-taught programmers or a person with a 4-year CS degree, we were never taught how we go from basic logic gates to CPUs.\nA book that I have come across- The Elements of Computing Systems by By Noam Nisan and Shimon Schocken does just that. It teaches you how the basic building blocks of modern computers actually create the computers.\n\n\n\nnand2tetris-book-cover-2ed\n\n\n[Although it shows the new 2nd edition, I am reading the first one]\nIt promises to teach you, actively, how we can start from nothing but a NAND gate and go all the way to a full-fledged computer with an operating system, and a general-purpose programming language that runs on it, which can be used to create anything. The book is also known as nand2tetris.\nI have started reading this book and working through it every Sunday, and I have almost finished a chapter.\n\n\nBuilding Gates from Elementary Gates\nPeople who have taken a Digital Architecture class or a Digital Electronics class will know that NAND (NOT AND) gates are called “universal gates” because some combination of them are able to create any other gates. So, when you have a NAND gate with you, you can create a general-purpose computer with it. And the book makes you do that. It makes you create many logic gates starting from AND, NOT, and OR with nothing but a pre-implemented NAND gate.\nSo, a NAND gate is already implemented and supplied. You first have to create AND, OR, and NOT gates. Then you go on to further create XOR gates, Multiplexors (MUX), Demultiplexors (D-MUX), and multi-bit versions of these gates and basic gates with what you have built previously, viz. AND, OR, NOT, and NAND gates.\nThis is a fascinating task in itself, and very intellectually stimulating.\nThis is where the core of the post comes in.\nI will show you how to implement any Boolean function with nothing but three logic gates. This is a trick worth knowing.\nLet’s start.\n\n\n\nImplementing a Boolean Function\n\nDesign a Boolean Function\nLet’s say we need a Boolean function that behaves in such a way-\n\n\n\nA\nB\nC\nY\n\n\n\n\n0\n0\n0\n1\n\n\n0\n0\n1\n0\n\n\n0\n1\n0\n0\n\n\n0\n1\n1\n0\n\n\n1\n0\n0\n1\n\n\n1\n0\n1\n1\n\n\n1\n1\n0\n1\n\n\n1\n1\n1\n0\n\n\n\nThis truth table tells you that we need such a Boolean function that outputs 1 iff- * A, B, and C, three Boolean variables, are all 0 * only A is 1, B and C are 0 * A and C are 1, B is 0 * A and B are 1, C is 0\nNow, in some situations, you might need to define a Boolean function that behaves as you expect. Do not worry about what this function is doing. Let’s focus on the implementation part.\n\n\nBoolean Expression\nI am assuming that you already know about the basic gates, and how they function.\nSo, in the first step to build a Boolean function, you must create the Boolean expression. Just like a Boolean variable can have two values (0, 1), and a Boolean function can only output two values (0, 1, duh!), a Boolean expression always evaluates to either 0 or 1.\nHow to actually do it?\n\nYou should just note in which cases the function outputs a 1. Focus just on those.\nNote which variables are on (1) and which are off (0).\nKeep the variables unchanged which are 1 in this case, and take a negation of those which are off (in electronics, you would put them through NOT).\nMultiply them together.\nFor these products for each 1 in the output, just add them together.\n\nThat’s it. You have got your Boolean expression.\nLet me go through it step by step.\nIn the first occurrence of 1 in the output, we see that all the input variables are off. So, we get a*, b*, and c*, where a, b, and c are the variables representing inputs A, B, and C, respectively, and x* is the negation of x. We multiply them together, and we get a*b*c*.\nIn the second occurrence of 1, in a similar manner, we get- ab*c*.\nFor the third and fourth occurrences of 1, we get ab*c, and abc*, respectively.\nWe have to add them. And doing so, we get-\n\\[ y = \\overline{a}\\overline{b}\\overline{c} + a\\overline{b}\\overline{c} + a\\overline{b}c + ab\\overline{c} \\]\n[You are required to write cases in the proper order. The first of three variable has these values row-wise- 0-0-0-0-1-1-1-1, the second one varies- 0-0-1-1-0-0-1-1, and the third one- 0-1-0-1-0-1-0-1.]\n\n\nImplementation\nYou might be already aware that negation is passing through a NOT gate, multiplying is just passing through an AND gate, and adding is just an OR gate.\nThe first component a*b*c* can be obtained in this way-\n\n\n\nimage.png\n\n\nAnd here are the second, third, and fourth components-\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\nAnd,\n\n\n\nimage.png\n\n\nNow, we have to add them all together, i.e. put these components through an OR gate. The output of the OR gate will be our final output.\n\n\n\nimage.png\n\n\nOur final output.\n\n\nHDL: Implement and Test\nNow, if you have ICs lying around, you can quickly test this with a power supply or a 5V cell, and some wires. You will also need a multimeter or LED bulbs to check your output.\nBut, remember, we are building a computer from scratch when following this book. Using thousands of ICs to build a computer is not slightly practical unless you happen to have a contract with TSMC!\nSomething called HDL (Hardware Description Language) is used. It is an intuitive, high-level, special-purpose programming language that you use to design and test chips, and use previously created chips.\nIt is implemented in Java, but you do not have to worry about its implementation, you just need to use it. A GUI program is also supplied to do load and test chips.\nFor example, given a NAND gate, you will design an AND gate this way.\n\n\n\nimage.png\n\n\nAs you can see, it is very intuitive, and you can pick it up in 20 minutes.\nAnd, this is how you’d test this:\n\n\n\nimage.png\n\n\nTo write our custom function in HDL, we would do-\n// Custom.hdl\n// this is a part of Hashnode blog Convoluted demo\n\nCHIP Custom {\n    IN a, b, c;\n    OUT out;\n\n    PARTS:\n    Not(in=a, out=nota);\n    Not(in=b, out=notb);\n    Not(in=c, out=notc);\n    And(a=nota, b=notb, out=notanotb);\n    And(a=notanotb, b=notc, out=w1);\n    And(a=a, b=notb, out=anotb);\n    And(a=anotb, b=notc, out=w2);\n    And(a=anotb, b=c, out=w3);\n    And(a=a, b=b, out=ab);\n    And(a=ab, b=notc, out=w4);\n    Or8Way(in[0]=w1, in[1]=w2, in[2]=w3, in[3]=w4, out=out);\n}\nAnd as this is a custom chip, the .tst test file, and .cmp compare file are not supplied. So, for testing this chip, I wrote my own.\nHere is the test (.tst) file-\n// Custom.tst\n// test file for testing custom chip\n// could be found at- https://gist.github.com/ghosh-r/c4e6f5ceb1e7ea2e3ba3601c9de121be\n\n// test file for a custom chip in Convoluted, a Hashnode blog\n\nload Custom.hdl,\noutput-file Custom.out,\ncompare-to Custom.cmp,\noutput-list a%B3.1.3 b%B3.1.3 c%B3.1.3 out%B3.1.3;\n\nset a 0,\nset b 0,\nset c 0,\neval,\noutput;\n\nset a 0,\nset b 0,\nset c 1,\neval,\noutput;\n\nset a 0,\nset b 1,\nset c 0,\neval,\noutput;\n\nset a 0,\nset b 1,\nset c 1,\neval,\noutput; \n\nset a 1,\nset b 0,\nset c 0,\neval,\noutput;\n\nset a 1,\nset b 0,\nset c 1,\neval,\noutput;\n\nset a 1,\nset b 1,\nset c 0,\neval,\noutput;\n\nset a 1,\nset b 1,\nset c 1,\neval,\noutput;\nAnd here is the compare (.cmp) file. It contains the truth table that we expect.\n// Custom.cmp\n// compare file for chip Custom.hdl\n// found at- https://gist.github.com/ghosh-r/c4e6f5ceb1e7ea2e3ba3601c9de121be\n// test file at- https://gist.github.com/ghosh-r/cef52b9f6ac017e00d64460b025a53fe\n\n|   a   |   b   |   c   |  out  |\n|   0   |   0   |   0   |   1   |\n|   0   |   0   |   1   |   0   |\n|   0   |   1   |   0   |   0   |\n|   0   |   1   |   1   |   0   |\n|   1   |   0   |   0   |   1   |\n|   1   |   0   |   1   |   1   |\n|   1   |   1   |   0   |   1   |\n|   1   |   1   |   1   |   0   |\nNote that the text in Compare files is not robust to whitespaces.\nIt will be compared with the output file generated by the simulator.\nHere’s how the successful run looks like-\n\n\n\nfinal_60c5cd572911290063ac8055_216317.gif\n\n\n\n\nAnother Example: XOR Gate\nSuppose you want to implement a XOR gate from basic gates. And the truth-table would be-\n\n\n\nA\nB\nY\n\n\n\n\n0\n0\n0\n\n\n0\n1\n1\n\n\n1\n0\n1\n\n\n1\n1\n0\n\n\n\nThis will be your Boolean expression-\n\\[ y = \\overline{a}b + a\\overline{b} \\]\nAnd this will be your HDL implementation.\n// Xor.hdl\n\nCHIP Xor {\n    IN a, b;\n    OUT out;\n\n    PARTS:\n    Not(in=a, out=nota);\n    Not(in=b, out=notb);\n    And(a=a, b=notb, out=w1);\n    And(a=nota, b=b, out=w2);\n    Or(a=w1, b=w2, out=out);\n}\n\n\n\n\nConclusion\nI showed you how to implement any Boolean function with nothing but three elementary logic gates. However, you should keep in mind that this approach is impractical when there are more variables, and the chip you want is more complicated.\n\nFollow the blog to get more similar posts.\nFollow me on Twitter or connect with me on LinkedIn."
  }
]