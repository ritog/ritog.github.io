[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Writings on matters of Deep Learning, functional programming, programming languages, and fun projects."
  },
  {
    "objectID": "posts/nand2tetris_1/2021-06-13-chips-logic-gates-nand2tetris-1.html",
    "href": "posts/nand2tetris_1/2021-06-13-chips-logic-gates-nand2tetris-1.html",
    "title": "Creating Any Boolean Function",
    "section": "",
    "text": "by Ritobrata Ghosh\n\nIntroduction\nDigital electronics is probably the most impactful and widespread technology now. It forms the basis of all of the chips- from ARM microcontrollers to processors in cellphones, all the way to uberpowerful AMD Threadrippers. In all these cases, digital electronics reign supreme.\nIt is very useful to learn not only about basic principles but also how these principles and the components of Digital Electronics exactly form the basis of the modern world, i.e., how do we go from basic logic gates to fully functional computers.\n\n\nElements of Computing Systems\nDigital Electronics is not only important because it is the base of the modern technology that literally governs our lives, but it makes us better as programmers, and computer scientists.\nWhether we are self-taught programmers or a person with a 4-year CS degree, we were never taught how we go from basic logic gates to CPUs.\nA book that I have come across- The Elements of Computing Systems by By Noam Nisan and Shimon Schocken does just that. It teaches you how the basic building blocks of modern computers actually create the computers.\n\n\n\nnand2tetris-book-cover-2ed\n\n\n[Although it shows the new 2nd edition, I am reading the first one]\nIt promises to teach you, actively, how we can start from nothing but a NAND gate and go all the way to a full-fledged computer with an operating system, and a general-purpose programming language that runs on it, which can be used to create anything. The book is also known as nand2tetris.\nI have started reading this book and working through it every Sunday, and I have almost finished a chapter.\n\n\nBuilding Gates from Elementary Gates\nPeople who have taken a Digital Architecture class or a Digital Electronics class will know that NAND (NOT AND) gates are called “universal gates” because some combination of them are able to create any other gates. So, when you have a NAND gate with you, you can create a general-purpose computer with it. And the book makes you do that. It makes you create many logic gates starting from AND, NOT, and OR with nothing but a pre-implemented NAND gate.\nSo, a NAND gate is already implemented and supplied. You first have to create AND, OR, and NOT gates. Then you go on to further create XOR gates, Multiplexors (MUX), Demultiplexors (D-MUX), and multi-bit versions of these gates and basic gates with what you have built previously, viz. AND, OR, NOT, and NAND gates.\nThis is a fascinating task in itself, and very intellectually stimulating.\nThis is where the core of the post comes in.\nI will show you how to implement any Boolean function with nothing but three logic gates. This is a trick worth knowing.\nLet’s start.\n\n\n\nImplementing a Boolean Function\n\nDesign a Boolean Function\nLet’s say we need a Boolean function that behaves in such a way-\n\n\n\nA\nB\nC\nY\n\n\n\n\n0\n0\n0\n1\n\n\n0\n0\n1\n0\n\n\n0\n1\n0\n0\n\n\n0\n1\n1\n0\n\n\n1\n0\n0\n1\n\n\n1\n0\n1\n1\n\n\n1\n1\n0\n1\n\n\n1\n1\n1\n0\n\n\n\nThis truth table tells you that we need such a Boolean function that outputs 1 iff- * A, B, and C, three Boolean variables, are all 0 * only A is 1, B and C are 0 * A and C are 1, B is 0 * A and B are 1, C is 0\nNow, in some situations, you might need to define a Boolean function that behaves as you expect. Do not worry about what this function is doing. Let’s focus on the implementation part.\n\n\nBoolean Expression\nI am assuming that you already know about the basic gates, and how they function.\nSo, in the first step to build a Boolean function, you must create the Boolean expression. Just like a Boolean variable can have two values (0, 1), and a Boolean function can only output two values (0, 1, duh!), a Boolean expression always evaluates to either 0 or 1.\nHow to actually do it?\n\nYou should just note in which cases the function outputs a 1. Focus just on those.\nNote which variables are on (1) and which are off (0).\nKeep the variables unchanged which are 1 in this case, and take a negation of those which are off (in electronics, you would put them through NOT).\nMultiply them together.\nFor these products for each 1 in the output, just add them together.\n\nThat’s it. You have got your Boolean expression.\nLet me go through it step by step.\nIn the first occurrence of 1 in the output, we see that all the input variables are off. So, we get a*, b*, and c*, where a, b, and c are the variables representing inputs A, B, and C, respectively, and x* is the negation of x. We multiply them together, and we get a*b*c*.\nIn the second occurrence of 1, in a similar manner, we get- ab*c*.\nFor the third and fourth occurrences of 1, we get ab*c, and abc*, respectively.\nWe have to add them. And doing so, we get-\n\\[ y = \\overline{a}\\overline{b}\\overline{c} + a\\overline{b}\\overline{c} + a\\overline{b}c + ab\\overline{c} \\]\n[You are required to write cases in the proper order. The first of three variable has these values row-wise- 0-0-0-0-1-1-1-1, the second one varies- 0-0-1-1-0-0-1-1, and the third one- 0-1-0-1-0-1-0-1.]\n\n\nImplementation\nYou might be already aware that negation is passing through a NOT gate, multiplying is just passing through an AND gate, and adding is just an OR gate.\nThe first component a*b*c* can be obtained in this way-\n\n\n\nimage.png\n\n\nAnd here are the second, third, and fourth components-\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\nAnd,\n\n\n\nimage.png\n\n\nNow, we have to add them all together, i.e. put these components through an OR gate. The output of the OR gate will be our final output.\n\n\n\nimage.png\n\n\nOur final output.\n\n\nHDL: Implement and Test\nNow, if you have ICs lying around, you can quickly test this with a power supply or a 5V cell, and some wires. You will also need a multimeter or LED bulbs to check your output.\nBut, remember, we are building a computer from scratch when following this book. Using thousands of ICs to build a computer is not slightly practical unless you happen to have a contract with TSMC!\nSomething called HDL (Hardware Description Language) is used. It is an intuitive, high-level, special-purpose programming language that you use to design and test chips, and use previously created chips.\nIt is implemented in Java, but you do not have to worry about its implementation, you just need to use it. A GUI program is also supplied to do load and test chips.\nFor example, given a NAND gate, you will design an AND gate this way.\n\n\n\nimage.png\n\n\nAs you can see, it is very intuitive, and you can pick it up in 20 minutes.\nAnd, this is how you’d test this:\n\n\n\nimage.png\n\n\nTo write our custom function in HDL, we would do-\n// Custom.hdl\n// this is a part of Hashnode blog Convoluted demo\n\nCHIP Custom {\n    IN a, b, c;\n    OUT out;\n\n    PARTS:\n    Not(in=a, out=nota);\n    Not(in=b, out=notb);\n    Not(in=c, out=notc);\n    And(a=nota, b=notb, out=notanotb);\n    And(a=notanotb, b=notc, out=w1);\n    And(a=a, b=notb, out=anotb);\n    And(a=anotb, b=notc, out=w2);\n    And(a=anotb, b=c, out=w3);\n    And(a=a, b=b, out=ab);\n    And(a=ab, b=notc, out=w4);\n    Or8Way(in[0]=w1, in[1]=w2, in[2]=w3, in[3]=w4, out=out);\n}\nAnd as this is a custom chip, the .tst test file, and .cmp compare file are not supplied. So, for testing this chip, I wrote my own.\nHere is the test (.tst) file-\n// Custom.tst\n// test file for testing custom chip\n// could be found at- https://gist.github.com/ghosh-r/c4e6f5ceb1e7ea2e3ba3601c9de121be\n\n// test file for a custom chip in Convoluted, a Hashnode blog\n\nload Custom.hdl,\noutput-file Custom.out,\ncompare-to Custom.cmp,\noutput-list a%B3.1.3 b%B3.1.3 c%B3.1.3 out%B3.1.3;\n\nset a 0,\nset b 0,\nset c 0,\neval,\noutput;\n\nset a 0,\nset b 0,\nset c 1,\neval,\noutput;\n\nset a 0,\nset b 1,\nset c 0,\neval,\noutput;\n\nset a 0,\nset b 1,\nset c 1,\neval,\noutput; \n\nset a 1,\nset b 0,\nset c 0,\neval,\noutput;\n\nset a 1,\nset b 0,\nset c 1,\neval,\noutput;\n\nset a 1,\nset b 1,\nset c 0,\neval,\noutput;\n\nset a 1,\nset b 1,\nset c 1,\neval,\noutput;\nAnd here is the compare (.cmp) file. It contains the truth table that we expect.\n// Custom.cmp\n// compare file for chip Custom.hdl\n// found at- https://gist.github.com/ghosh-r/c4e6f5ceb1e7ea2e3ba3601c9de121be\n// test file at- https://gist.github.com/ghosh-r/cef52b9f6ac017e00d64460b025a53fe\n\n|   a   |   b   |   c   |  out  |\n|   0   |   0   |   0   |   1   |\n|   0   |   0   |   1   |   0   |\n|   0   |   1   |   0   |   0   |\n|   0   |   1   |   1   |   0   |\n|   1   |   0   |   0   |   1   |\n|   1   |   0   |   1   |   1   |\n|   1   |   1   |   0   |   1   |\n|   1   |   1   |   1   |   0   |\nNote that the text in Compare files is not robust to whitespaces.\nIt will be compared with the output file generated by the simulator.\nHere’s how the successful run looks like-\n\n\n\nfinal_60c5cd572911290063ac8055_216317.gif\n\n\n\n\nAnother Example: XOR Gate\nSuppose you want to implement a XOR gate from basic gates. And the truth-table would be-\n\n\n\nA\nB\nY\n\n\n\n\n0\n0\n0\n\n\n0\n1\n1\n\n\n1\n0\n1\n\n\n1\n1\n0\n\n\n\nThis will be your Boolean expression-\n\\[ y = \\overline{a}b + a\\overline{b} \\]\nAnd this will be your HDL implementation.\n// Xor.hdl\n\nCHIP Xor {\n    IN a, b;\n    OUT out;\n\n    PARTS:\n    Not(in=a, out=nota);\n    Not(in=b, out=notb);\n    And(a=a, b=notb, out=w1);\n    And(a=nota, b=b, out=w2);\n    Or(a=w1, b=w2, out=out);\n}\n\n\n\n\nConclusion\nI showed you how to implement any Boolean function with nothing but three elementary logic gates. However, you should keep in mind that this approach is impractical when there are more variables, and the chip you want is more complicated.\n\nFollow the blog to get more similar posts.\nFollow me on Twitter or connect with me on LinkedIn.\n\n\n\n\nReuseCC-BY-NC-SA"
  },
  {
    "objectID": "posts/intro-to-huggingface-api/2021-06-20-intro-huggingface-api.html",
    "href": "posts/intro-to-huggingface-api/2021-06-20-intro-huggingface-api.html",
    "title": "A Gentle Introduction to the Hugging Face API",
    "section": "",
    "text": "Introduction\nNatural Language Processing is a fast-advancing field. And it is also one of the fields that require a huge amount of computational resources to make important progress. And although breakthroughs are openly announced, and papers are released in free-to-access repositories such as arXiv, Open Review, Papers with Code, etc., and despite (sometimes) having the code freely available on GitHub, using those language models is not something widely accessible and easy.\nLet me provide more context. BERT is a state-of-the-art encoder language model. It takes days to train the model from the ground up even when using very powerful GPUs that only a few entities have access to. In 2019, NVIDIA used 1472 NVIDIA V100 GPUs to train BERT from scratch in 53 minutes. Yes, 1,472!\nOne estimate puts the cost of training GPT-3, a 175 billion parameter model, for a single training run at $12 Million USD.\nAnd such language models are released every now and then. How do you use these powerful language models for your task?\nHere Hugging Face comes to the scene. They aim to solve this problem by providing pre-trained models, and simple API so that you can use them, fine-tune them, and use the API in your applications.\nIn this article, my goal is to introduce the Hugging Face pipeline API to accomplish very interesting tasks by utilizing powerful pre-trained models present in the models hub of Hugging Face.\nTo follow through this article, you need not have any prior knowledge of Natural Language Processing. I, however, assume minor prior experience in writing Python code.\n\nIn this article, I will go over, describe, and provide examples for the following tasks using Hugging Face pipeline-\n\nSentiment Analysis\nZero-Shot Classification\nText Generation\nMask-Filling\nNamed Entity Recognition\nQuestion Answering\nSummarization\nTranslation\n\n\n\n\nInstalling and Importing\n! pip install transformers[sentencepiece] &gt; /dev/null\nIf you have not seen a pip install with a square bracket in it before, don’t worry. It means that you are installing the transformers package with extra support added for the sentencepiece package. You can find out more about square parentheses installs in pip here.\nfrom transformers import pipeline\nWe will use the utilities available in the pipeline namespace in the transformers library.\n\n\n\nSentiment Analysis\nBefore I begin going through the specific pipelines, let me tell you something beforehand that you will find yourself. Hugging Face API is very intuitive. When you want to use a pipeline, you have to instantiate an object, then you pass data to that object to get result. Very simple! You are soon to see what I mean.\nclassifier_sentiment = pipeline(\"sentiment-analysis\")\nThat’s it. You call the pipeline() method with the task you want to accomplish as an argument. And you assign a name to it. You are done now. You can now begin to use the object as a function to achieve what you want. Let’s see an example-\n\n\nSee, it returns a dictionary contained in a list that has two items, label and score. The label part tells us its prediction, and the score tells us its confidence score.\n\nAs an aside, I think they are structured this way because this structure is easily compatible with .json and similar filetypes which are very common in APIs.\n\nLet’s see another example.\n\n\nSee, how the classifier is aware of my emotion about pineapple pizzas? 🙃\n\n\n\nZero-Shot Classification\nWhen you want to classify something using Deep Learning, in many cases you need to train it with labeled examples. This approach is known as Supervised Learning. Even when leveraging transfer learning, you need to train your model with quite a few labeled examples in the domain of your choice. Zero-shot classification is different than that. In this, you use a pre-trained model to classify a given string and some labels of your choice. The model returns you the confidence score for each model.\nLet me show you how this works. First, you instantiate an object and assign it a name just like you did in the case of the Sentiment Analysis pipeline.\nclassifier_zero_shot = pipeline(\"zero-shot-classification\")\nThen you pass a string, along with the labels of your choice to test how well they correspond to your sentence.\n\n\nSee, the given sentence is about a profession, and the model tells you that. And it knows that it is much more related to education than politics.\nEssentially, when you are doing Zero-Shot Classification, you are supplying a string to the pipeline, and also labels. The pipeline returns how accurate those labels are.\n\n\n\nText Generation\nBefore I heard about Deep Learning, I heard about Natural Language Processing, because I heard that you can generate text with it! This is the most exciting part of NLP to me, personally.\nAs the name suggests, this pipeline lets you generate text. It just needs you to supply a prompt, a text that will determine what the generated text will be. Let me show you how.\ngenerator = pipeline(\"text-generation\")\n\n\nSee what I mean! Even with zero training, it can create logically coherent text that is even interesting to read. Robots will take over writers soon 🤖!\nYou can control the maximum length of the output of the pipeline, and even ask for multiple possible outputs, like so-\n\n\n\nWhoa, it’s poetic!\n\nWe can accomplish more interesting things with transfer learning. But that is a story for another day.\n\n\n\nMask Filling\nIn this pipeline, if there is a word that you hide and supply the string with the hidden word, the pipeline will predict that word. This is like magic!\nunmasker = pipeline(\"fill-mask\")\nYou pass the sentence with the masked word, and it will predict it. You can choose the number of predictions you want to see. You just have to pass a value to the top_k parameter.\n\n\n\n\nIt can predict the words quite well. Just remember to mask the word using &lt;mask&gt;.\n\n\n\nNamed Entity Recognition\nThis pipeline recognizes the proper nouns in your sentence. And it also classifies them. Let’s see an example.\nner = pipeline(\"ner\", grouped_entities=True)\nThe grouped_entities parameter just ensures that the pipeline can recognize the names that have more than one word, like, you know, Chocolate Factory.\n\n\nSee how it recognizes a person, his profession, and location on its own, and also labels them as “PER”, “ORG”, and “LOC” (meaning location), respectively.\n\n\n\nQuestion Answering\nThis pipeline provides you with an answer to a question you ask from the context that you provide.\nquestion_answerer = pipeline(\"question-answering\")\n\n\nIt can answer multiple questions from the same prompt.\n\n\n\n\n\nSummarization\nThis is one of the most practical tasks that has existed for a long time. Deep Learning really changes the landscape of the task.\nLet’s see it in action.\nsummarizer = pipeline(\"summarization\")\nLet’s summarize a piece of text from a blog post I recently read.\n\n\nNow, that’s something! The efficacy of the summarizer really shocked me. It does this task so well.\n\n\n\nTranslation\nThe translation pipeline translates a piece of text from one language to another. It also lets you choose the language model of your choice.\nFirst, let’s translate a text from English to German. I will use the Helsinki-NLP/opus-mt-en-de model for this task.\ntranslator_de = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-de\")\n\n\nNote that you are not explicitly telling the model about the language of the input language or the output language. A language model is only trained for translating from one language to another. It cannot translate other languages. If you want to translate from German to English, you’d have to use another model from the model hub. You can find all the translation models here.\nThe cool thing is that Hugging Face is not limited to Romance languages or European languages in general. Let me show you by translating a piece of text from Bengali.\ntranslator_bn = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-bn-en\")\n\n\n\n\n\n\n\n\nConclusion\nI have shown you several pipeline APIs and what they do and how they work.\nHere are the suggested next steps-\n\nExperiment. Play with the pipelines by yourself, try different things with different parameters, run inference on examples of your own, test edge cases, run wild with it. I believe that is the best way to learn.\nDo Projects. Use the knowledge you gained here in simple projects. We only learn by doing.\nHugging Face Course. Hugging Face has recently released a free course where they teach you about Transformers and their library ecosystem. It is a good next step.\nExplore. Explore the Hugging Face website, learn more about their APIs, Services, pre-trained models, and many more things it has to offer.\n\n\nIf you notice any error, mistake, or typo, please feel free to let me know. If something is not clear, please let me know that as well. I will do my best to correct it and will credit you here.\nLet me know if you have any questions at all.\n\n(Originally published as a Notebook on Kaggle.)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nNon-Linear Dynamics Trajectories with Python\n\n\n\n\n\n\n\ndeep-learning\n\n\nphysics\n\n\nchaos-theory\n\n\nai\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2023\n\n\nRitobrata Ghosh\n\n\n\n\n\n\n  \n\n\n\n\nStuff About Complex Number I Wish Somebody Told Me\n\n\nStuff About Complex Number I Wish I Knew Earlier\n\n\n\n\nmath\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2022\n\n\nRitobrata Ghosh\n\n\n\n\n\n\n  \n\n\n\n\nA Gentle Introduction to the Hugging Face API\n\n\nA Hands-On Tutorial\n\n\n\n\ndeep-learning\n\n\nmachine-learning\n\n\ndata-science\n\n\nai\n\n\ntutorial\n\n\nnlp\n\n\nkaggle\n\n\n\n\n\n\n\n\n\n\n\nJun 20, 2021\n\n\nRitobrata Ghosh\n\n\n\n\n\n\n  \n\n\n\n\nCreating Any Boolean Function\n\n\nHow to Represent Any Boolean Function with Elementary Logic Gates\n\n\n\n\nnand2tetris\n\n\ncomputer-science\n\n\nelectronics\n\n\ncomputation\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2021\n\n\nRitobrata Ghosh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "now.html",
    "href": "now.html",
    "title": "Now",
    "section": "",
    "text": "What am I Doing Now?\nWhat is it about?\n\nTech Related\n\nDelving deep into Computational Neuroscience- applying AI to Comp. Neuro.\nLearning more about functional languages via learning Haskell, and reading FP books like Bird, Wadler.\nDelving into the lower level of SWE, learning about embedded systems, Assembly, etc.\nNow and then, I look into projects in the JAX ecosystem and following SciML projects.\nLooking into PhD positions in Comp. Neuro. and looking for RA jobs.\nSometimes tinkering with TinyML projects.\n\n\n\nOthers\n\nLearning to play the ukulele\nWriting a memoir about my parents’ childhoods\nReading books and writing reviews\nWatching non-mainstream movies\n\nLast updated: 30 Aug, 2024"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Jul 15, 2023 | Analytics Vidhya | Complete Introductory Guide to Speech to Text with Transformers | Link\nApr 22, 2023 | Zenodo | Non-Linear Dynamics Through Linear Algebraic Lenses: Attempting to Learn the Trajectories of the Logistic Map with Artificial Neural Networks | Link\nApr 5, 2021 | KDNuggets | Best Podcasts for Machine Learning | Link\nSep 29, 2020 | Towards Data Science | TAKING JULIA FOR A TEST DRIVE: My First Encounter With Julia | Link\nJul 31, 2020 | Towards Data Science | How Much Math Do You Need to Know to Get Started with Data Science? A Clear, Straightforward Answer | Link"
  },
  {
    "objectID": "TILs_index.html",
    "href": "TILs_index.html",
    "title": "TILs",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n2023-Jul-07\n\n\n\n\n\n\n\nfunctional-programming\n\n\nhaskell\n\n\ntil\n\n\n\n\n\n\n\n\n\n\n\nJul 7, 2023\n\n\nRitobrata Ghosh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "Worked in single-label classification with many labels.\nCleaned, wrangled a large amount of data (&gt;60 GB).\nSet up end-to-end Deep Learning training pipeline for single-label image classification.\nExperimented and worked with Deep Learning architectures such as DenseNet, ResNet, EfficientNet\nUsed improvement techniques such as image data augmentation, oversampling and undersampling, gradual freezing, discriminative learning rates, ensemble methods, etc.\nAchieved accuracy of &gt;91% in the classification task, where this has never been done before.\nWorked with tabular data of ~200k rows involving features to predict numerical data with XGBoost. Achieved ~98% accuracy.\nWorked closely with business stakeholders to understand and solve problems.\nStack: Python, PyTorch, fast.ai, Scikit-Learn, XGBoost, Pandas, Python, Colab Pro.\n\nSkills: Machine Learning, Deep Learning, Python (Programming Language), Data Science, Programming, Computer Vision, Linux, Jupyter Notebook, PyTorch, Written Communication, Convolutional Neural Networks (CNN)\n\n\n\n\n\n\n\nWrote the patent filing for a novel architecture developed inside the company (filing process ongoing)\nBriefly worked on Active Learning techniques in Computer Vision to automate the training process and improve accuracy and maintenance.\nStack: Python, PyTorch, fast.ai, AWS SageMaker, AWS S3, Pandas.\n\nSkills: Deep Learning, Linux, AWS SageMaker, Team Work\n\n\n\n\n\n\n\nReviewing a book on Advanced PyTorch. Chapters include: multimodal AI, Deep Reinforcement Learning, etc.\nWorking with co-ordinators and other editors.\n\nSkills: PyTorch, Computer Vision, NLP, Reinforcement Learning, Team Work\n\n\n\n\n\n\n\nThis online course was offered by Stanford University during the COVID-19 pandemic. It brought together 12,000 students and 1,100 volunteer teachers participating from around the world. The course is a 6-week introduction to Python programming using materials from the first half of Stanford’s CS106A course.\nAs a volunteer section leader, I prepared and taught a weekly discussion section of 8-10 students to supplement professors’ lectures.\nReceived direct training from Stanford Professors Mehran Sahami, Chris Piech, and Julie Zelenski. Received further training from Stanford-appointed teacher mentors.\nTaught introductory Python and CS to about 12 students from India, Bangladesh, and the USA. Students were of various ages and came from diverse backgrounds including from Fine Arts backgrounds. For many students, it was the first introduction to programming and CS ever.\nTaught five sessions of my own (lasting 1.5 h on average) and a backup session for another.\nTaught programming, problem-solving, and CS concepts.\n\nSkills: Python, Programming, Teaching, Team Work\n\n\n\n\n\n\n\n\n\n\nCGPA: 9.04/10 Notable courses: Advanced algorithms, Soft Computing, Artificial Intelligence\n\n\n\n\n\n\nGrade: First Class Notable courses: Mathematical Methods for Physics I, II, Quantum Mechanics, Statistical Mechanics, Special Theory of Relativity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHuman brain is responsible for both motor tasks, i.e. when actual movement of some body parts happen, and also for imagery tasks: when the person imagines a movement. This is a project that aims to find functional connectivity among brain areas in both imagined and real motor movements. This project also aims to find the similarity (or lack thereof) between real and imaginary movement in lower dimensional latent space of brain signals using Principal Component Analysis.\n\nSkills: Python, PyTorch, Deep Learning, Computational Neuroscience, Team Work\n\n\n\n\n\n\n\n\n\nI introduce the JAX library for solving basic problems in Machine Learning. I explain everything so that a beginner in this framework will find it helpful. Introduced the functional programming paradigm for Deep Learning.\nWon the Google Open Source Software Expert Prize\nSkills: JAX, Machine Learning, Deep Learning, Written Communication\n\n\n\n\n\n\n\n\n\n\nThis project is an Open-Source Deep Learning architecture that generates captions for satellite imagery.\nWe used transfer-learning to fine-tune OpenAI CLIP for satellite data.\nTraining was done for multiple hours on a TPUv3-8 enabled virtual machine provided by Google Cloud.\nWe used JAX/Flax and PyTorch for this model.\nGitHub: https://github.com/arampacha/CLIP-rsicd\nDemo: https://huggingface.co/spaces/sujitpal/clip-rsicd-demo\nI worked on researching and applying different augmentation techniques that led to the improvement of the model.This project is an Open-Source Deep Learning architecture that generates captions for satellite imagery. We used transfer-learning to fine-tune OpenAI CLIP for satellite data. Training was done for multiple hours on a TPUv3-8 enabled virtual machine provided by Google Cloud. We used JAX/Flax and PyTorch for this model.\n\nSkills: Transfer Learning, Deep Learning, Computer Vision, Linux, Jupyter Notebook\n\n\n\n\n\n\n\n\n\n\nThis project is an Open-Source Deep Learning architecture that generates images from a given text prompt. This implements the functionality of OpenAI’s DALL-E architecture, but with a 27x smaller model.\nA BART autoregressive model was used to process and generate text, and a VQGAN to generate the images. Then we used a pre-trained CLIP model from OpenAI to rank the generated images.\nTraining was done for 70 hours on a TPUv3-8 enabled virtual machine provided by Google Cloud.\nWe used JAX/Flax and PyTorch for this model.\nGitHub: https://github.com/borisdayma/dalle-mini\nDemo: https://huggingface.co/spaces/flax-community/dalle-mini\n\nSkills: Deep Learning, Linux, Google Cloud Platform, JAX, Flax, Team Work\n\n\n\n\n\n\n\n\n\n\nTrained a Convolutional Neural Network with ResNet-34 leveraging transfer learning. Deployed the model as a web app for inference.\nThis app distinguishes between Kathakali and Chhau- two classical dance forms of India.Trained a Convolutional Neural Network with ResNet-34 leveraging transfer learning. Deployed the model as a web app for inference. GitHub- https://github.com/ghosh-r/folkAI This app distinguishes between Kathakali and Chhau- two classical dance forms of India.\n\nSkills: Transfer Learning, Deep Learning, Computer Vision\n\n\n\n\n\n\n\n\n\n\nApplied KMeans Clustering algorithm to cluster counties of California, to look for and recommend new investment opportunities for investors looking to fund new eateries in California City.\nEvaluated feature correlation through the Pearson Coefficient, and evaluated model through elbow method.\nFinal Report Link: https://github.com/ghosh-r/California_Eatery/blob/master/final_deliverables/capstone_project_final_report.pdf\n\nSkills: Machine Learning, Data Science, Python, Scikit-Learn, Written Communication\n\n\n\n\n\n\n\n\n\n\nI scraped about 2,700 copyright-free Bengali poems from the web and made them available under a permissive license. The intention of the project is to provide a freely available, well-structured dataset for downstream tasks of pre-trained language models, such as poetry generation.\nThis dataset is ideal for training language models to generate prose with pre-trained language models such as GPT-2, GPT-J, etc.\nDOI: 10.34740/KAGGLE/DSV/2400728\n\nSkills: Linux, Python, Web Scraping\n\n\n\n\n\n\n\n\n\n\nCreated a Dataset containing 70,000 images from an existing database so that it can be used for a seamless introduction to Vision, and can be used to solve commercial problems involving Bengali handwritten digits.\n\nSkills: Python, Linux\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this world-wide competition, I was awarded the prize along with two others for demonstrating expertise in Google’s Open Source software JAX and communicating my knowledge.\n\n\n\n\n\n\n\n\n\n\n\n80+ Teams of about 800 people took part in this competition, and among ~50 final projects, our project was picked as the top one.\nOur team consisted of a Software Engineer, two ML Engineers, three Ph.D. Students, and two undergraduate students.\nWe built an Open Source version of OpenAI’s Dall-E Mini, that generates images from text. We used JAX/Flax as our framework, trained on Cloud TPU-VMs with TPUv3-8s. We used a VQGAN + BART architecture, and CLIP to rank generated images.\nI personally worked on sequence generation using BART and created part of the model front-end using Streamlit. Actively took part in all other aspects of the project.\nOn the Jury were Ross Wightman (Investor, DL Library maintainer), Asish Vaswani (of Transformers), Niki Parmar (Research Scientist, Google Brain), and Thomas Wolf (CSO of Hugging Face).\nGitHub: https://github.com/borisdayma/dalle-mini\nReport: https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini–Vmlldzo4NjIxODA\n\n\n\n\n\n\n\n\n\n\n\n80+ Teams of about 800 people took part in this competition, and among ~50 final projects, our project was picked for third place.\nWe built an Open Source model for captioning satellite images by fine-tuning CLIP with satellite image data (RSICD dataset). We expect it to be used in natural disaster management, climate monitoring, and defense among other uses.\nWe used JAX/Flax as our framework, trained on Cloud TPU-VMs with one TPUv3-8.\nI personally worked on data augmentation techniques for the images in the dataset for improving the accuracy score.\nOn the Jury were Ross Wightman (Investor, DL Library maintainer), Ashish Vaswani (of Transformers fame), Niki Parmar (Research Scientist, Google Brain), and Thomas Wolf (CSO, Hugging Face).\nGitHub: https://lnkd.in/eeWQBiX\nReport: https://lnkd.in/en74Mmk\n\n\n\n\n\n\n\n\n\n\n\nWas awarded the the “Best Speaker of Legislative Assembly” award in district level of Youth Parliament Competition in (then undivided) Burdwan District, West Bengal. Received certificate signed by the Hon’ble Minister-in-Charge of Departments of Parliamentary Affairs, School Education and Higher Education, Government of West Bengal and Shri Basudeb Banerjee, I.A.S, Additional Chief Secretary, Home & Parliamentary Affairs Department, Government of West Bengal.\nYouth Parliament Competition is a Government funded competition to grow awarness about Parliamentary Democratic process, rules and decorum among school students.\nMy school stood first at the block level, and I was awarded the same award in the Block level and qualified to District level, and received the award.\n\n\n\n\n\n\n\n\n\n\n\nDesigned a model of a stradling bus, and discussed on its logistics and was awarded 3rd Place in District Level (then undivided) Burdwan District.\nAnd participated in State Level of the Competition.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrote an introductory guide to using Transformers models for solving audio-related problems.\nIt is very practical and one can get started with Audio Machine Learning right away after reading this blog. This also teaches how to use Hugging Face to find models, datasets, and how to use them to solve one’s own problems.\n\n\n\n\n\n\n\n\n\n\n\nTo automatically learn the behavior of trajectories of a map in Non-Linear Dynamics- the Logistic Map, Deep Neural Networks have been trained. Different iterates of the Logistic Map have been generated and models have been fit to them to test the learning capabilities of Neural Networks under such scenario. This paper examines the capability of Neural Networks to learn the dynamics of a system that can be modeled with the Logistic Map.\nkeywords- Non-Linear Dynamics, Deep Learning, Artificial Neural Networks, Physics, Computational Mathematics, Logistic Map\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI recorded my voice and validates others’ for creating the Mozilla Common Voice Dataset. I was involved in five languages- Bengali, English, Spanish, Hindi, and German.\n\n\n\n\n\n\n\nI help answer questions from beginners getting into programming and learning Data Science. I also guide beginners about programming languages, problem solving skills, tools, and other related things about programming and Data Science.\n\n\n\n\n\n\n\n\nServed as a volunteer in Porichoy, a local group of volunteers aimed to serve the less fortunate after natural disasters and helping in education of children from economically challenged families. I mainly worked in rehabilitation after flood as a part of a team and organised cultural events.\n\n\n\n\n\n\n\n\nTrained a group of fifteen high school students in Youth Parliament Competition.\nTaught them about government, state-level polity, and conducts of Vidhan Sabha (State Legisslative Assembly)\nHelped train them in speech delivery, acting, etc.\nHelped write the script of 45 minute run- many additions, full editing, and wrote all English dialogues.\nIn the first competition, held at the level of Municipality, they won the First place beating three other schools.\nIn the next level, they won the Third place competing with 29 other schools in the District Level.\n\n\n\n\n\n\n\n\nProject Bhasha was aimed at creating a website and a community around it to teach Bengali to foreigners. We took a broader approach to teach culture as well as the language. My role was to write articles on Bengali art, culture, language, and literature. I also oversaw some aspects of the overall strategy, pedagogy, etc.\n\n\n\n\n\n\n\nReading books, swimming, playing the piano, narrating stories, etc."
  },
  {
    "objectID": "cv.html#experience",
    "href": "cv.html#experience",
    "title": "CV",
    "section": "",
    "text": "Worked in single-label classification with many labels.\nCleaned, wrangled a large amount of data (&gt;60 GB).\nSet up end-to-end Deep Learning training pipeline for single-label image classification.\nExperimented and worked with Deep Learning architectures such as DenseNet, ResNet, EfficientNet\nUsed improvement techniques such as image data augmentation, oversampling and undersampling, gradual freezing, discriminative learning rates, ensemble methods, etc.\nAchieved accuracy of &gt;91% in the classification task, where this has never been done before.\nWorked with tabular data of ~200k rows involving features to predict numerical data with XGBoost. Achieved ~98% accuracy.\nWorked closely with business stakeholders to understand and solve problems.\nStack: Python, PyTorch, fast.ai, Scikit-Learn, XGBoost, Pandas, Python, Colab Pro.\n\nSkills: Machine Learning, Deep Learning, Python (Programming Language), Data Science, Programming, Computer Vision, Linux, Jupyter Notebook, PyTorch, Written Communication, Convolutional Neural Networks (CNN)\n\n\n\n\n\n\n\nWrote the patent filing for a novel architecture developed inside the company (filing process ongoing)\nBriefly worked on Active Learning techniques in Computer Vision to automate the training process and improve accuracy and maintenance.\nStack: Python, PyTorch, fast.ai, AWS SageMaker, AWS S3, Pandas.\n\nSkills: Deep Learning, Linux, AWS SageMaker, Team Work\n\n\n\n\n\n\n\nReviewing a book on Advanced PyTorch. Chapters include: multimodal AI, Deep Reinforcement Learning, etc.\nWorking with co-ordinators and other editors.\n\nSkills: PyTorch, Computer Vision, NLP, Reinforcement Learning, Team Work\n\n\n\n\n\n\n\nThis online course was offered by Stanford University during the COVID-19 pandemic. It brought together 12,000 students and 1,100 volunteer teachers participating from around the world. The course is a 6-week introduction to Python programming using materials from the first half of Stanford’s CS106A course.\nAs a volunteer section leader, I prepared and taught a weekly discussion section of 8-10 students to supplement professors’ lectures.\nReceived direct training from Stanford Professors Mehran Sahami, Chris Piech, and Julie Zelenski. Received further training from Stanford-appointed teacher mentors.\nTaught introductory Python and CS to about 12 students from India, Bangladesh, and the USA. Students were of various ages and came from diverse backgrounds including from Fine Arts backgrounds. For many students, it was the first introduction to programming and CS ever.\nTaught five sessions of my own (lasting 1.5 h on average) and a backup session for another.\nTaught programming, problem-solving, and CS concepts.\n\nSkills: Python, Programming, Teaching, Team Work"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "CV",
    "section": "",
    "text": "CGPA: 9.04/10 Notable courses: Advanced algorithms, Soft Computing, Artificial Intelligence\n\n\n\n\n\n\nGrade: First Class Notable courses: Mathematical Methods for Physics I, II, Quantum Mechanics, Statistical Mechanics, Special Theory of Relativity"
  },
  {
    "objectID": "cv.html#projects",
    "href": "cv.html#projects",
    "title": "CV",
    "section": "",
    "text": "Human brain is responsible for both motor tasks, i.e. when actual movement of some body parts happen, and also for imagery tasks: when the person imagines a movement. This is a project that aims to find functional connectivity among brain areas in both imagined and real motor movements. This project also aims to find the similarity (or lack thereof) between real and imaginary movement in lower dimensional latent space of brain signals using Principal Component Analysis.\n\nSkills: Python, PyTorch, Deep Learning, Computational Neuroscience, Team Work\n\n\n\n\n\n\n\n\n\nI introduce the JAX library for solving basic problems in Machine Learning. I explain everything so that a beginner in this framework will find it helpful. Introduced the functional programming paradigm for Deep Learning.\nWon the Google Open Source Software Expert Prize\nSkills: JAX, Machine Learning, Deep Learning, Written Communication\n\n\n\n\n\n\n\n\n\n\nThis project is an Open-Source Deep Learning architecture that generates captions for satellite imagery.\nWe used transfer-learning to fine-tune OpenAI CLIP for satellite data.\nTraining was done for multiple hours on a TPUv3-8 enabled virtual machine provided by Google Cloud.\nWe used JAX/Flax and PyTorch for this model.\nGitHub: https://github.com/arampacha/CLIP-rsicd\nDemo: https://huggingface.co/spaces/sujitpal/clip-rsicd-demo\nI worked on researching and applying different augmentation techniques that led to the improvement of the model.This project is an Open-Source Deep Learning architecture that generates captions for satellite imagery. We used transfer-learning to fine-tune OpenAI CLIP for satellite data. Training was done for multiple hours on a TPUv3-8 enabled virtual machine provided by Google Cloud. We used JAX/Flax and PyTorch for this model.\n\nSkills: Transfer Learning, Deep Learning, Computer Vision, Linux, Jupyter Notebook\n\n\n\n\n\n\n\n\n\n\nThis project is an Open-Source Deep Learning architecture that generates images from a given text prompt. This implements the functionality of OpenAI’s DALL-E architecture, but with a 27x smaller model.\nA BART autoregressive model was used to process and generate text, and a VQGAN to generate the images. Then we used a pre-trained CLIP model from OpenAI to rank the generated images.\nTraining was done for 70 hours on a TPUv3-8 enabled virtual machine provided by Google Cloud.\nWe used JAX/Flax and PyTorch for this model.\nGitHub: https://github.com/borisdayma/dalle-mini\nDemo: https://huggingface.co/spaces/flax-community/dalle-mini\n\nSkills: Deep Learning, Linux, Google Cloud Platform, JAX, Flax, Team Work\n\n\n\n\n\n\n\n\n\n\nTrained a Convolutional Neural Network with ResNet-34 leveraging transfer learning. Deployed the model as a web app for inference.\nThis app distinguishes between Kathakali and Chhau- two classical dance forms of India.Trained a Convolutional Neural Network with ResNet-34 leveraging transfer learning. Deployed the model as a web app for inference. GitHub- https://github.com/ghosh-r/folkAI This app distinguishes between Kathakali and Chhau- two classical dance forms of India.\n\nSkills: Transfer Learning, Deep Learning, Computer Vision\n\n\n\n\n\n\n\n\n\n\nApplied KMeans Clustering algorithm to cluster counties of California, to look for and recommend new investment opportunities for investors looking to fund new eateries in California City.\nEvaluated feature correlation through the Pearson Coefficient, and evaluated model through elbow method.\nFinal Report Link: https://github.com/ghosh-r/California_Eatery/blob/master/final_deliverables/capstone_project_final_report.pdf\n\nSkills: Machine Learning, Data Science, Python, Scikit-Learn, Written Communication\n\n\n\n\n\n\n\n\n\n\nI scraped about 2,700 copyright-free Bengali poems from the web and made them available under a permissive license. The intention of the project is to provide a freely available, well-structured dataset for downstream tasks of pre-trained language models, such as poetry generation.\nThis dataset is ideal for training language models to generate prose with pre-trained language models such as GPT-2, GPT-J, etc.\nDOI: 10.34740/KAGGLE/DSV/2400728\n\nSkills: Linux, Python, Web Scraping\n\n\n\n\n\n\n\n\n\n\nCreated a Dataset containing 70,000 images from an existing database so that it can be used for a seamless introduction to Vision, and can be used to solve commercial problems involving Bengali handwritten digits.\n\nSkills: Python, Linux"
  },
  {
    "objectID": "cv.html#honors-and-awards",
    "href": "cv.html#honors-and-awards",
    "title": "CV",
    "section": "",
    "text": "In this world-wide competition, I was awarded the prize along with two others for demonstrating expertise in Google’s Open Source software JAX and communicating my knowledge.\n\n\n\n\n\n\n\n\n\n\n\n80+ Teams of about 800 people took part in this competition, and among ~50 final projects, our project was picked as the top one.\nOur team consisted of a Software Engineer, two ML Engineers, three Ph.D. Students, and two undergraduate students.\nWe built an Open Source version of OpenAI’s Dall-E Mini, that generates images from text. We used JAX/Flax as our framework, trained on Cloud TPU-VMs with TPUv3-8s. We used a VQGAN + BART architecture, and CLIP to rank generated images.\nI personally worked on sequence generation using BART and created part of the model front-end using Streamlit. Actively took part in all other aspects of the project.\nOn the Jury were Ross Wightman (Investor, DL Library maintainer), Asish Vaswani (of Transformers), Niki Parmar (Research Scientist, Google Brain), and Thomas Wolf (CSO of Hugging Face).\nGitHub: https://github.com/borisdayma/dalle-mini\nReport: https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini–Vmlldzo4NjIxODA\n\n\n\n\n\n\n\n\n\n\n\n80+ Teams of about 800 people took part in this competition, and among ~50 final projects, our project was picked for third place.\nWe built an Open Source model for captioning satellite images by fine-tuning CLIP with satellite image data (RSICD dataset). We expect it to be used in natural disaster management, climate monitoring, and defense among other uses.\nWe used JAX/Flax as our framework, trained on Cloud TPU-VMs with one TPUv3-8.\nI personally worked on data augmentation techniques for the images in the dataset for improving the accuracy score.\nOn the Jury were Ross Wightman (Investor, DL Library maintainer), Ashish Vaswani (of Transformers fame), Niki Parmar (Research Scientist, Google Brain), and Thomas Wolf (CSO, Hugging Face).\nGitHub: https://lnkd.in/eeWQBiX\nReport: https://lnkd.in/en74Mmk\n\n\n\n\n\n\n\n\n\n\n\nWas awarded the the “Best Speaker of Legislative Assembly” award in district level of Youth Parliament Competition in (then undivided) Burdwan District, West Bengal. Received certificate signed by the Hon’ble Minister-in-Charge of Departments of Parliamentary Affairs, School Education and Higher Education, Government of West Bengal and Shri Basudeb Banerjee, I.A.S, Additional Chief Secretary, Home & Parliamentary Affairs Department, Government of West Bengal.\nYouth Parliament Competition is a Government funded competition to grow awarness about Parliamentary Democratic process, rules and decorum among school students.\nMy school stood first at the block level, and I was awarded the same award in the Block level and qualified to District level, and received the award.\n\n\n\n\n\n\n\n\n\n\n\nDesigned a model of a stradling bus, and discussed on its logistics and was awarded 3rd Place in District Level (then undivided) Burdwan District.\nAnd participated in State Level of the Competition."
  },
  {
    "objectID": "cv.html#publications",
    "href": "cv.html#publications",
    "title": "CV",
    "section": "",
    "text": "Wrote an introductory guide to using Transformers models for solving audio-related problems.\nIt is very practical and one can get started with Audio Machine Learning right away after reading this blog. This also teaches how to use Hugging Face to find models, datasets, and how to use them to solve one’s own problems.\n\n\n\n\n\n\n\n\n\n\n\nTo automatically learn the behavior of trajectories of a map in Non-Linear Dynamics- the Logistic Map, Deep Neural Networks have been trained. Different iterates of the Logistic Map have been generated and models have been fit to them to test the learning capabilities of Neural Networks under such scenario. This paper examines the capability of Neural Networks to learn the dynamics of a system that can be modeled with the Logistic Map.\nkeywords- Non-Linear Dynamics, Deep Learning, Artificial Neural Networks, Physics, Computational Mathematics, Logistic Map"
  },
  {
    "objectID": "cv.html#volunteering-experiences",
    "href": "cv.html#volunteering-experiences",
    "title": "CV",
    "section": "",
    "text": "I recorded my voice and validates others’ for creating the Mozilla Common Voice Dataset. I was involved in five languages- Bengali, English, Spanish, Hindi, and German.\n\n\n\n\n\n\n\nI help answer questions from beginners getting into programming and learning Data Science. I also guide beginners about programming languages, problem solving skills, tools, and other related things about programming and Data Science.\n\n\n\n\n\n\n\n\nServed as a volunteer in Porichoy, a local group of volunteers aimed to serve the less fortunate after natural disasters and helping in education of children from economically challenged families. I mainly worked in rehabilitation after flood as a part of a team and organised cultural events.\n\n\n\n\n\n\n\n\nTrained a group of fifteen high school students in Youth Parliament Competition.\nTaught them about government, state-level polity, and conducts of Vidhan Sabha (State Legisslative Assembly)\nHelped train them in speech delivery, acting, etc.\nHelped write the script of 45 minute run- many additions, full editing, and wrote all English dialogues.\nIn the first competition, held at the level of Municipality, they won the First place beating three other schools.\nIn the next level, they won the Third place competing with 29 other schools in the District Level.\n\n\n\n\n\n\n\n\nProject Bhasha was aimed at creating a website and a community around it to teach Bengali to foreigners. We took a broader approach to teach culture as well as the language. My role was to write articles on Bengali art, culture, language, and literature. I also oversaw some aspects of the overall strategy, pedagogy, etc."
  },
  {
    "objectID": "cv.html#hobbies",
    "href": "cv.html#hobbies",
    "title": "CV",
    "section": "",
    "text": "Reading books, swimming, playing the piano, narrating stories, etc."
  },
  {
    "objectID": "posts/complex-numbers/index.html",
    "href": "posts/complex-numbers/index.html",
    "title": "Stuff About Complex Number I Wish Somebody Told Me",
    "section": "",
    "text": "by Rito Ghosh\n\n\nThere are many places that teach you complex number like High School, college classes, YouTube channels, and so on.\nBut they earlier ones often heavy on solving problems by-hand, and almost every time, they do only that.\nThe latter ones focus mainly on shiny visuals or advanced properties that are not intended for the uninitiated.\nThis is a piece written for both people who are getting introduced to Complex Numbers for the first time, and for the people who have studied it in college, and are hazy on the topic. (Why won’t they be? They had to focus on solving boring problems by hand.)\nWhen I saw Jeremy talk about Complex Numbers on the APL study group, I knew I had to write something- I did my Junior year Math paper on Complex Number in High School!\nThe idea is to:\n\nquickly introduce Complex Numbers, and how to work with them\ntalk about their origins: why need complex numbers in the first place?\nshow how they extend the number line\nshow how \\(i\\) works as an operator, and briefly what operators are\ntalk about a real world example, and probable future usage\n\nI will keep it short and crisp.\n\n\n\n\nComplex Numbers, as you might have seen are written like this:\n\\[x = a + ib\\]\nHere, the whole thing in the right-hand side is the number. a is the Real part, and b is the imaginary part.\nDo not get misguided by the names. Imaginary numbers aren’t imaginary at all. They make appearance, or rather, they can represent stuff in real life really well! We will soon see.\nYou can add and subtract complex numbers. You add (or subtract) the real and imaginary parts separately.\nLet me show you.\n\\[(4 + 5i) + (3 + 6i) = (4 + 3) + (5 + 6)i\\]\n\\[ (6i) - (3 + 7i) = (0-3) - (7-6)i = -3 - i\\]\nThey can be multiplied and divided with real numbers and other complex numbers. They work just as you expect them to.\n\\[(3 + 2i)(1 + 4i) = 3 + 12i + 2i + 8i^2\\]\nLet’s not spend too much time here.\nAs how they behave, and more related information can quickly be found- and good ones at that- on internet.\nI suggest the Khan Academy course on Complex Numbers.\n\nOh, I haven’t told you the value of \\(i\\) yet.\n\\(i\\), is defined such that,\n\\[i = \\sqrt(-1), \\text{or}, i^2 = -1\\]\nThat’s it!\n\n\n\n\nThe story goes, when the Greeks were ridden with famines and other troubles, the Oracle of Delphi told them to double the height of the cube made in honor of Apollo.\nThe Greeks could not do it! How could they?\nTo double the size of a cube, you need to find the cube root of 2. The Greeks didn’t know how to that!\nToo bad.\nYou are probably familiar with the story that Pythagoras, frustrated with the fact that the the size of the diagonal of a square with the side of 1 could not be measured, ordered to off someone! Just like that!\nBecause the square root of 2 was not measurable then. It is not a number that you write down like the others.\nHuman beings needed something that could be tamed- numbers that always had roots- no matter what the number was!\nThis is how it came to be. Complex numbers are numbers that always has roots. No matter what.\nAnd with the advent of imaginary numbers, it was possible to calculate the square root of negative numbers, too. But that came later.\n\n\n\n\nThe number line is a simple animal.\nIt is one-dimensional. It extends both ways of zero.\nAnd thus, it was limiting.\nComplex numbers was defined in such a way that they added one extra dimension to the number line.\nIt wasn’t so one-dimensional anymore.\n\n\n\nargand plane\n\n\nPicture: Wikipedia\nNow, numbers weren’t limited to a line, but it was a plane.\nAnd many more operations became possible that simply weren’t before.\n\n\n\n\ni is not simple. Adding it or multiplying it has consequences.\nIf you don’t know yet, very simply, operators are stuff that act on other stuff.\n(No mathematical rigor whatsoever can be expected from the previous line)\nYou know operators. We are talking about it in APL. We use them whenever we code.\nFunctions are also operators.\nThink, for example, about the - operator.\nThis is our friendly negative sign. But add it before a positive number, and the result will be a negative number. The number will no longer remain positive.\nThink about the + operator in math. It takes two numbers, not one. And the result is the sum of both of them.\nOperator takes stuff in it, and spits out results.\ni is also an operator.\nMultiplying a number with i rotates the number by 90 degrees counter-clockwise in the complex plain.\nRotate it twice, and you get the negative of a number. Because, you know, i is the square of -1. And multiplying a number with square of i negates it, i.e. rotates the number by 180 degrees, or throws it to the dark, ehm, negative side.\nThis has important consequences.\n\n\n\n\nThe scope of this piece is very limited.\nBut I will show you one example.\nAlternating Current is not steady, or fixed like Direct Current or DC. AC current, the one powers our home is the only practical form of electricity that can be practically transported from one point to another over a long distance.\nThis form of electricity acts like a wave.\nAnd whenever waves are involved, we have to deal with angles, frequencies, and amplitudes.\nWhen angles become involved, we can use trigonometric functions like cosine and sine.\nAnd complex numbers are related to trigonometric functions via the Euler’s Formula:\n\\[e^{i \\theta} = \\cos{\\theta} + i \\cdot \\sin{\\theta}\\]\nAnd, when we can use the special number, e, we can solve many problems easily because the function returns itself after differentiation and integration.\nThat makes a lot of the things easier.\nWhile this is the reason for using complex numbers in electronics in the first place, they also make life easier for easier calculation and quantity manipulation.\n\n\n\n\nYou might think that complex numbers are such nuisance, and we could have done the same clever things with adding one more dimension to the real number like we do in geometry. Would not a ‘y’ suffice instead of this dimension riddled with ‘imaginary numbers’?\nThe answer is no.\nYou will not have the same behavior then.\nYou won’t be able to get the niceties that is afforded to you via Euler’s formula and the access it provides you to the exponential function.\n\nI hope I was able to give you some intuition about complex numbers, and their uses.\nPlease let me know what you think.\n\nSubscribe to my substack to get posts on your inbox, or subscribe via RSS."
  },
  {
    "objectID": "posts/complex-numbers/index.html#introduction",
    "href": "posts/complex-numbers/index.html#introduction",
    "title": "Stuff About Complex Number I Wish Somebody Told Me",
    "section": "",
    "text": "There are many places that teach you complex number like High School, college classes, YouTube channels, and so on.\nBut they earlier ones often heavy on solving problems by-hand, and almost every time, they do only that.\nThe latter ones focus mainly on shiny visuals or advanced properties that are not intended for the uninitiated.\nThis is a piece written for both people who are getting introduced to Complex Numbers for the first time, and for the people who have studied it in college, and are hazy on the topic. (Why won’t they be? They had to focus on solving boring problems by hand.)\nWhen I saw Jeremy talk about Complex Numbers on the APL study group, I knew I had to write something- I did my Junior year Math paper on Complex Number in High School!\nThe idea is to:\n\nquickly introduce Complex Numbers, and how to work with them\ntalk about their origins: why need complex numbers in the first place?\nshow how they extend the number line\nshow how \\(i\\) works as an operator, and briefly what operators are\ntalk about a real world example, and probable future usage\n\nI will keep it short and crisp."
  },
  {
    "objectID": "posts/complex-numbers/index.html#comples-numbers",
    "href": "posts/complex-numbers/index.html#comples-numbers",
    "title": "Stuff About Complex Number I Wish Somebody Told Me",
    "section": "",
    "text": "Complex Numbers, as you might have seen are written like this:\n\\[x = a + ib\\]\nHere, the whole thing in the right-hand side is the number. a is the Real part, and b is the imaginary part.\nDo not get misguided by the names. Imaginary numbers aren’t imaginary at all. They make appearance, or rather, they can represent stuff in real life really well! We will soon see.\nYou can add and subtract complex numbers. You add (or subtract) the real and imaginary parts separately.\nLet me show you.\n\\[(4 + 5i) + (3 + 6i) = (4 + 3) + (5 + 6)i\\]\n\\[ (6i) - (3 + 7i) = (0-3) - (7-6)i = -3 - i\\]\nThey can be multiplied and divided with real numbers and other complex numbers. They work just as you expect them to.\n\\[(3 + 2i)(1 + 4i) = 3 + 12i + 2i + 8i^2\\]\nLet’s not spend too much time here.\nAs how they behave, and more related information can quickly be found- and good ones at that- on internet.\nI suggest the Khan Academy course on Complex Numbers.\n\nOh, I haven’t told you the value of \\(i\\) yet.\n\\(i\\), is defined such that,\n\\[i = \\sqrt(-1), \\text{or}, i^2 = -1\\]\nThat’s it!"
  },
  {
    "objectID": "posts/complex-numbers/index.html#why-do-we-need-i-and-the-whole-complex-number-scenery",
    "href": "posts/complex-numbers/index.html#why-do-we-need-i-and-the-whole-complex-number-scenery",
    "title": "Stuff About Complex Number I Wish Somebody Told Me",
    "section": "",
    "text": "The story goes, when the Greeks were ridden with famines and other troubles, the Oracle of Delphi told them to double the height of the cube made in honor of Apollo.\nThe Greeks could not do it! How could they?\nTo double the size of a cube, you need to find the cube root of 2. The Greeks didn’t know how to that!\nToo bad.\nYou are probably familiar with the story that Pythagoras, frustrated with the fact that the the size of the diagonal of a square with the side of 1 could not be measured, ordered to off someone! Just like that!\nBecause the square root of 2 was not measurable then. It is not a number that you write down like the others.\nHuman beings needed something that could be tamed- numbers that always had roots- no matter what the number was!\nThis is how it came to be. Complex numbers are numbers that always has roots. No matter what.\nAnd with the advent of imaginary numbers, it was possible to calculate the square root of negative numbers, too. But that came later."
  },
  {
    "objectID": "posts/complex-numbers/index.html#extending-the-number-line",
    "href": "posts/complex-numbers/index.html#extending-the-number-line",
    "title": "Stuff About Complex Number I Wish Somebody Told Me",
    "section": "",
    "text": "The number line is a simple animal.\nIt is one-dimensional. It extends both ways of zero.\nAnd thus, it was limiting.\nComplex numbers was defined in such a way that they added one extra dimension to the number line.\nIt wasn’t so one-dimensional anymore.\n\n\n\nargand plane\n\n\nPicture: Wikipedia\nNow, numbers weren’t limited to a line, but it was a plane.\nAnd many more operations became possible that simply weren’t before."
  },
  {
    "objectID": "posts/complex-numbers/index.html#i-as-an-operator",
    "href": "posts/complex-numbers/index.html#i-as-an-operator",
    "title": "Stuff About Complex Number I Wish Somebody Told Me",
    "section": "",
    "text": "i is not simple. Adding it or multiplying it has consequences.\nIf you don’t know yet, very simply, operators are stuff that act on other stuff.\n(No mathematical rigor whatsoever can be expected from the previous line)\nYou know operators. We are talking about it in APL. We use them whenever we code.\nFunctions are also operators.\nThink, for example, about the - operator.\nThis is our friendly negative sign. But add it before a positive number, and the result will be a negative number. The number will no longer remain positive.\nThink about the + operator in math. It takes two numbers, not one. And the result is the sum of both of them.\nOperator takes stuff in it, and spits out results.\ni is also an operator.\nMultiplying a number with i rotates the number by 90 degrees counter-clockwise in the complex plain.\nRotate it twice, and you get the negative of a number. Because, you know, i is the square of -1. And multiplying a number with square of i negates it, i.e. rotates the number by 180 degrees, or throws it to the dark, ehm, negative side.\nThis has important consequences."
  },
  {
    "objectID": "posts/complex-numbers/index.html#example-of-use-of-complex-number-in-real-world-electronics",
    "href": "posts/complex-numbers/index.html#example-of-use-of-complex-number-in-real-world-electronics",
    "title": "Stuff About Complex Number I Wish Somebody Told Me",
    "section": "",
    "text": "The scope of this piece is very limited.\nBut I will show you one example.\nAlternating Current is not steady, or fixed like Direct Current or DC. AC current, the one powers our home is the only practical form of electricity that can be practically transported from one point to another over a long distance.\nThis form of electricity acts like a wave.\nAnd whenever waves are involved, we have to deal with angles, frequencies, and amplitudes.\nWhen angles become involved, we can use trigonometric functions like cosine and sine.\nAnd complex numbers are related to trigonometric functions via the Euler’s Formula:\n\\[e^{i \\theta} = \\cos{\\theta} + i \\cdot \\sin{\\theta}\\]\nAnd, when we can use the special number, e, we can solve many problems easily because the function returns itself after differentiation and integration.\nThat makes a lot of the things easier.\nWhile this is the reason for using complex numbers in electronics in the first place, they also make life easier for easier calculation and quantity manipulation."
  },
  {
    "objectID": "posts/complex-numbers/index.html#why-bother-isnt-one-more-dimension-enough",
    "href": "posts/complex-numbers/index.html#why-bother-isnt-one-more-dimension-enough",
    "title": "Stuff About Complex Number I Wish Somebody Told Me",
    "section": "",
    "text": "You might think that complex numbers are such nuisance, and we could have done the same clever things with adding one more dimension to the real number like we do in geometry. Would not a ‘y’ suffice instead of this dimension riddled with ‘imaginary numbers’?\nThe answer is no.\nYou will not have the same behavior then.\nYou won’t be able to get the niceties that is afforded to you via Euler’s formula and the access it provides you to the exponential function.\n\nI hope I was able to give you some intuition about complex numbers, and their uses.\nPlease let me know what you think.\n\nSubscribe to my substack to get posts on your inbox, or subscribe via RSS."
  },
  {
    "objectID": "posts/non-linear-dynamics-trajectories-with-python/index.html",
    "href": "posts/non-linear-dynamics-trajectories-with-python/index.html",
    "title": "Non-Linear Dynamics Trajectories with Python",
    "section": "",
    "text": "Non-Linear Dynamics Trajectories with Python\nby Ritobrata Ghosh\n\nIntroduction\nIn this Notebook, trajectories of the Logistic Map [1] is plotted with Python.\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef logistic_function(parameter: float, initial_value: float) -&gt; float:\n    return parameter * initial_value * (1 - initial_value)\ndef logistic_function_trajectory(parameter: float,\n                                 initial_value: float,\n                                 num_iter: int) -&gt; np.array:\n    trajectory = np.zeros(num_iter)\n    trajectory[0] = initial_value\n    for i in range(num_iter-1):\n        trajectory[i+1] = logistic_function(parameter, initial_value)\n        initial_value = trajectory[i+1]\n    return np.array(trajectory)\nlogistic_function_trajectory(parameter=2.2, initial_value=0.8, num_iter=10)\narray([0.8       , 0.352     , 0.5018112 , 0.54999278, 0.54450159,\n       0.54564314, 0.54541675, 0.5454621 , 0.54545303, 0.54545485])\nxs = np.linspace(1, 200, 200)\nys = logistic_function_trajectory(parameter=2.2, initial_value=0.8, num_iter=200)\ndef plot_trajectory(xs, ys, x_label, y_label, title):\n    plt.figure(figsize=(20,10))\n    plt.scatter(xs, ys, marker='o', edgecolor='black', c=point_colors * 50);\n    plt.xlabel(x_label);\n    plt.ylabel(y_label);\n    plt.title(title);\n    plt.grid();\npoint_colors = ['cyan', 'magenta', 'yellow', 'black']\nplot_trajectory(xs=xs, ys=ys, x_label='$n$', y_label='$x_n$', title='Trajectory')\n\n\n\n\\[r = 2\\]\nx_n = logistic_function_trajectory(parameter=2.0, initial_value=0.2, num_iter=200)\nx_n_hat = logistic_function_trajectory(parameter=2.0, initial_value=0.200001, num_iter=200)\nys = abs(np.subtract(x_n, x_n_hat))\nxs = np.linspace(1, 200, 200)\nplot_trajectory(xs=xs, ys=ys, x_label='$n$', y_label='$ x_n - \\hat{x_n} $', title='Trajectory for $r = 2$')\n\n\n\nPlotting \\(x_n - \\hat{x_n}\\) vs. \\(n\\) for \\(r=3.4\\)\nx_n = logistic_function_trajectory(parameter=3.4, initial_value=0.2, num_iter=200)\nx_n_hat = logistic_function_trajectory(parameter=3.4, initial_value=0.200001, num_iter=200)\n\nys = abs(np.subtract(x_n, x_n_hat))\nxs = np.linspace(1, 200, 200)\n\nplot_trajectory(xs=xs, ys=ys, x_label='$n$', y_label='$ x_n - \\hat{x_n} $', title='Trajectory for $r = 3.4$')\n\nWe can thus plot the trajectories of the Logistic Map with Python and relevant libraries.\n\n\n\nReferences\n\nhttps://en.wikipedia.org/wiki/Logistic_map\nBradley, Liz: Nonlinear Dynamics: Mathematical and Computational Approaches via Complexity Explorer, Santa Fe Institute\n\nCite this Notebook:\n@ONLINE {,\n    author = \"Ritobrata Ghosh\",\n    title  = \"Non Linear Dynamics Trajectories with Python\",\n    month  = \"apr\",\n    year   = \"2023\",\n    url    = \"https://www.kaggle.com/truthr/non-linear-dynamics-trajectories-with-python\"\n}"
  },
  {
    "objectID": "TILs/2023-Jul-07/index.html",
    "href": "TILs/2023-Jul-07/index.html",
    "title": "2023-Jul-07",
    "section": "",
    "text": "Learned that procedures as data can also be seen and used in Haskell, as well as Scheme/LISP.\n\nghci&gt; head [(*3), (+10), square] 5\n15\nI got this from Dmitrii K.’s Haskell course on YouTube. video\n\n\n\nReuseCC-BY-NC-SA"
  }
]